# ğŸš€ Fraud Detection MLOps System

An end-to-end production-style fraud detection system built with:

- FastAPI
- XGBoost
- SHAP Explainability
- MLflow Experiment Tracking
- Drift Detection
- Docker (Multi-container setup)

This project focuses not just on model accuracy, but on **operationalizing machine learning systems**.

---

## ğŸ¯ Problem Context

The dataset contains only **0.17% fraud cases**, making it highly imbalanced.

Instead of optimizing only ROC-AUC (~0.97), this system focuses on:

- Threshold tuning
- Precision vs Recall tradeoffs
- False positive reduction
- Explainability
- Monitoring

---

## ğŸ“Š Model Performance

| Metric        | Value |
|---------------|--------|
| ROC-AUC       | ~0.968 |
| Best Threshold| 0.9238 |
| Precision     | ~94%   |
| Recall        | ~82%   |
| False Positives | 5    |

Threshold was optimized using F1-score from the Precision-Recall curve.

---

## ğŸ—ï¸ System Architecture

```mermaid
flowchart TD

    A[Client - Swagger / HTTP Request] --> B[FastAPI Inference Service]

    B --> C1[XGBoost Model (fraud_xgboost.pkl)]
    B --> C2[SHAP Explainer (TreeExplainer)]
    B --> C3[Drift Monitor (Z-Score Check)]

    C1 --> D[MLflow Tracking (Experiment Logging)]

    subgraph Docker Environment
        B
        C1
        C2
        C3
        D
    end
```
## âš™ï¸ API Endpoints

| Endpoint | Description |
|-----------|-------------|
| `/predict` | Predict fraud probability |
| `/predict_batch` | Batch prediction |
| `/explain` | SHAP explainability |
| `/metrics` | Model performance metrics |
| `/drift_check` | Feature distribution drift detection |

Swagger Docs:
```
http://localhost:8000/docs
```

---

## ğŸ§  Key Engineering Decisions

### 1ï¸âƒ£ Imbalanced Learning
Used `scale_pos_weight` in XGBoost to handle extreme class imbalance.

### 2ï¸âƒ£ Threshold Optimization
Instead of default 0.5, optimized threshold to 0.9238 for better precision-recall tradeoff.

### 3ï¸âƒ£ Explainability
Integrated SHAP TreeExplainer for feature-level contribution analysis.

### 4ï¸âƒ£ Drift Detection
Implemented feature drift detection using Z-score comparison against training distribution.

### 5ï¸âƒ£ Experiment Tracking
Tracked model training and metrics using MLflow.

### 6ï¸âƒ£ Containerization
Dockerized API and UI using Docker Compose for reproducible deployment.

---

## ğŸ³ Running with Docker

```bash
docker compose up --build
```

API will be available at:

```
http://localhost:8000
```

---

## ğŸ“ Project Structure

```
fraud-mlops-system/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/           # FastAPI service
â”‚   â””â”€â”€ ui/            # Streamlit dashboard (WIP)
â”‚
â”œâ”€â”€ training/          # Model training scripts
â”œâ”€â”€ monitoring/        # Drift detection logic
â”œâ”€â”€ models/            # Saved model artifacts
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ Dockerfile
```

---

## ğŸš€ Future Improvements

- Prometheus monitoring
- Logging middleware
- CI/CD pipeline (GitHub Actions)
- Cloud deployment (AWS / Render)
- Model versioning strategy
- Real-time streaming integration

---

## ğŸ“Œ Why This Project Matters

Most ML projects stop at model training.

This project focuses on:

- Deployment
- Monitoring
- Explainability
- Threshold optimization
- System design

It demonstrates production-minded machine learning engineering.

---

## ğŸ‘¤ Author

Raj Karthik  
ML / Data / AI Enthusiast 